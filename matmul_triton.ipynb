{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "E-mNhUjQuxNM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "N9lmLw8cuxNN"
      },
      "outputs": [],
      "source": [
        "def is_cuda():\n",
        "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eUMlpjFJuxNO"
      },
      "outputs": [],
      "source": [
        "def is_hip_mi200():\n",
        "    target = triton.runtime.driver.active.get_current_target()\n",
        "    return target.backend == 'hip' and target.arch == 'gfx90a'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "lBNGYaejuxNO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PA2 Part 2: MatMul+Relu+Add Fused Optimization.\n",
        "The kernel uses several optimization techniques:\n",
        "\n",
        "  1. Shared memory tiling.\n",
        "  2. Register tiling.\n",
        "  3. Cooperative fetching.\n",
        "  4. Operator Fusion\n",
        "  5. Write cache / epilogue fusion.\n",
        "\n",
        "Fill in the missing parts (marked with TODO).\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Tiling parameters - You will need to change these to achieve better results.\n",
        "# -----------------------------------------------------------------------------\n",
        "BLOCK_M = 128  # Tile size in the M dimension.\n",
        "BLOCK_N = 256 # Tile size in the N dimension.\n",
        "BLOCK_K = 32 # Tile size in the K dimension.\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Triton Kernel: Matrix Multiplication + ReLU + Add\n",
        "#\n",
        "# The kernel uses:\n",
        "#   Step 1: Tile assignment (each kernel computes a tile of C)\n",
        "#   Step 2: Shared memory tiling + Cooperative Fetching: Load tiles of A and B.\n",
        "#   Step 3: Register tiling: Use a register accumulator.\n",
        "#   Step 4: Add and ReLU fusion\n",
        "#   Step 5: Write cache/Epilogue: Write the final tile back to global memory.\n",
        "# -----------------------------------------------------------------------------\n",
        "# @triton.autotune(\n",
        "#     configs=get_cuda_autotune_config(),\n",
        "#     key=['M', 'N', 'K'],\n",
        "# )\n",
        "@triton.jit\n",
        "def matmul_add_relu_kernel_fp16(\n",
        "    a_ptr, b_ptr, c_ptr, d_ptr,\n",
        "    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n",
        "    stride_am: tl.constexpr, stride_ak: tl.constexpr,\n",
        "    stride_bk: tl.constexpr, stride_bn: tl.constexpr,\n",
        "    stride_cm: tl.constexpr, stride_cn: tl.constexpr,\n",
        "    stride_dm: tl.constexpr, stride_dn: tl.constexpr,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 1: Tile: Assignment\n",
        "    #\n",
        "    # Each kernel instance is mapped to a tile in the output matrix C.\n",
        "    # Compute the starting indices (m_start, n_start) for this tile.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Compute the tile indices using program_id(0) for M and program_id(1) for N.\n",
        "    pid1 = tl.program_id(0)\n",
        "    pid2 = tl.program_id(1)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 2: Register Tiling\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Initialize the accumulator \"acc\" with zeros (dtype: float16).\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float16)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 3: Shared Memory Tiling & Cooperative Fetching.\n",
        "    # Compute pointers to the sub-tiles of A and B that are needed to compute\n",
        "    # the current C tile. The offsets here serve to load BLOCK_M x BLOCK_K\n",
        "    # and BLOCK_K x BLOCK_N blocks from A and B respectively.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "    offs_m = pid1 * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid2 * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    a_ptrs = a_ptr + (offs_m[:, None] * stride_am)\n",
        "    b_ptrs = b_ptr + (offs_n[None, :] * stride_bn)\n",
        "\n",
        "\n",
        "    mask_a = (offs_m[:, None] < M)\n",
        "    mask_b = (offs_n[None, :] < N)\n",
        "\n",
        "    for k in range(0, K // BLOCK_K):\n",
        "        a = tl.load(a_ptrs + offs_k[None, :] * stride_ak, mask=mask_a & (offs_k[None, :] <  K - k * BLOCK_K), other=0.0)\n",
        "        b = tl.load(b_ptrs + offs_k[:, None] * stride_bk, mask=mask_b & (offs_k[:, None] <  K - k * BLOCK_K), other=0.0)\n",
        "        acc += tl.dot(a, b, out_dtype=tl.float16)\n",
        "\n",
        "        a_ptrs += BLOCK_K * stride_ak\n",
        "        b_ptrs += BLOCK_K * stride_bk\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 4: Apply ReLU and Add C to the accumulator\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)\n",
        "    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n",
        "    res = tl.maximum(0, acc + tl.load(c_ptrs, mask=mask, other=0.0))\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 5: Write Cache / Epilogue Fusion: Write the computed tile to D.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "    tl.store(d_ptr + (offs_m[:, None] * stride_dm + offs_n[None, :] * stride_dn), res, mask=mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "u16sz-IUuxNP"
      },
      "outputs": [],
      "source": [
        "def matmul_add_relu_fp16(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes Output = ReLU(A @ B + C) using fp16 precision for maximum throughput.\n",
        "    \"\"\"\n",
        "    M, K = a.shape\n",
        "    K2, N = b.shape\n",
        "    assert K == K2, \"Incompatible dimensions\"\n",
        "\n",
        "    d = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
        "    # Create launch grid\n",
        "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
        "\n",
        "    matmul_add_relu_kernel_fp16[grid](\n",
        "        a, b, c, d,\n",
        "        M, N, K,\n",
        "        a.stride(0), a.stride(1),\n",
        "        b.stride(0), b.stride(1),\n",
        "        c.stride(0), c.stride(1),\n",
        "        d.stride(0), d.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n",
        "    )\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "AJ7LlTPawPqB"
      },
      "outputs": [],
      "source": [
        "# Reference implementation using PyTorch\n",
        "def reference_matmul_add_relu(A, B, C):\n",
        "    result = torch.matmul(A, B).add(C).relu_()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4J5ZBpOuxNP",
        "outputId": "9db833cd-80b7-4e44-9330-06627a6c9a70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "triton_output_with_fp16_inputs=tensor([[ 0.0000,  6.1250,  0.0000,  ..., 10.0625,  0.0000,  0.0000],\n",
            "        [ 7.9102, 15.6328, 26.6094,  ..., 11.4609,  5.3750, 18.6250],\n",
            "        [ 2.7246,  0.0000,  0.0000,  ...,  0.0000, 26.0781,  0.0000],\n",
            "        ...,\n",
            "        [ 0.4448, 75.1875,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n",
            "        [ 6.9492,  1.1230,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [27.6094, 26.9531, 22.9219,  ..., 13.5391,  6.0508, 21.6250]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "torch_output_with_fp16_inputs=tensor([[ 0.0000,  6.1289,  0.0000,  ..., 10.0391,  0.0000,  0.0000],\n",
            "        [ 7.9102, 15.6328, 26.6250,  ..., 11.4531,  5.3945, 18.6562],\n",
            "        [ 2.7266,  0.0000,  0.0000,  ...,  0.0000, 26.1250,  0.0000],\n",
            "        ...,\n",
            "        [ 0.4316, 75.2500,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n",
            "        [ 6.9570,  1.1260,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [27.6406, 26.9531, 22.9375,  ..., 13.5625,  6.0391, 21.6406]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "✅ Triton and Torch match\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Accuracy Tests\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "    sz = 512\n",
        "    a = torch.randn((sz, sz), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    b = torch.randn((sz, sz), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    c = torch.randn((sz, sz), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    triton_output = matmul_add_relu_fp16(a, b, c)\n",
        "    torch_output = reference_matmul_add_relu(a, b, c)\n",
        "    print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
        "    print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
        "    rtol = 1e-2 if is_hip_mi200() else 0.032\n",
        "    if torch.allclose(triton_output, torch_output, atol=0.15, rtol=rtol):\n",
        "        print(\"✅ Triton and Torch match\")\n",
        "    else:\n",
        "        diff = triton_output - torch_output\n",
        "        abs_diff = torch.abs(diff)\n",
        "        max_abs_diff = torch.max(abs_diff)\n",
        "        print(f\"❌ Triton and Torch differ: {max_abs_diff=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj_dGOlazQJY",
        "outputId": "7129ef95-6f71-42ab-ec9e-13913f5f6085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.77 ms\n",
            "PyTorch implementation: 1.02 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.34x\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Performance Benchmark\n",
        "# IMPORTANT: DO NOT CHANGE THIS CODE.\n",
        "# THIS IS THE EXACT CODE THAT WILL BE USED TO GRADE YOUR IMPLEMENTATION.\n",
        "# ANY CHANGES TO THIS CODE (INCLUDING DIMENSIONS, REPEATS, etc.)\n",
        "# WILL CAUSE YOU TO HAVE DIFFERENT SPEEDUP RESULTS.\n",
        "# -----------------------------------------------------------------------------\n",
        "M = 2048\n",
        "K = 2048\n",
        "N = 2048\n",
        "\n",
        "# KEEP THESE MATRICES IN FP16. FP32 WILL NOT PROVIDE ACCURATE RESULTS\n",
        "A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "# warmup\n",
        "_ = matmul_add_relu_fp16(A, B, C)\n",
        "_ = reference_matmul_add_relu(A, B, C)\n",
        "\n",
        "REPEATS = 5000\n",
        "\n",
        "# time your implementation\n",
        "print(\"Triton implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = matmul_add_relu_fp16(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "triton_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "# time pytorch\n",
        "print(\"PyTorch implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = reference_matmul_add_relu(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "torch_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "print(f\"Performance comparison for matrix multiplication ({M}x{K} @ {K}x{N}):\")\n",
        "print(f\"Triton implementation: {triton_time*1000:.2f} ms\")\n",
        "print(f\"PyTorch implementation: {torch_time*1000:.2f} ms\")\n",
        "\n",
        "print(f\"\\nSpeedup of Triton vs PyTorch: {torch_time/triton_time:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB4-Kw3ZesaP",
        "outputId": "b648706b-6348-4efe-a055-6f32c5369d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 0\n",
            "Test 1\n",
            "Test 2\n",
            "Test 3\n",
            "Test 4\n",
            "Test 5\n",
            "Test 6\n",
            "Test 7\n",
            "Test 8\n",
            "Test 9\n",
            "Test 10\n",
            "Test 11\n",
            "Test 12\n",
            "Test 13\n",
            "Test 14\n",
            "Test 15\n",
            "Test 16\n",
            "Test 17\n",
            "Test 18\n",
            "Test 19\n",
            "Test 20\n",
            "Test 21\n",
            "Test 22\n",
            "Test 23\n",
            "Test 24\n",
            "Test 25\n",
            "Test 26\n",
            "Test 27\n",
            "Test 28\n",
            "Test 29\n",
            "Test 30\n",
            "Test 31\n",
            "Test 32\n",
            "Test 33\n",
            "Test 34\n",
            "Test 35\n",
            "Test 36\n",
            "Test 37\n",
            "Test 38\n",
            "Test 39\n",
            "Test 40\n",
            "Test 41\n",
            "Test 42\n",
            "Test 43\n",
            "Test 44\n",
            "Test 45\n",
            "Test 46\n",
            "Test 47\n",
            "Test 48\n",
            "Test 49\n",
            "Test 50\n",
            "Test 51\n",
            "Test 52\n",
            "Test 53\n",
            "Test 54\n",
            "Test 55\n",
            "Test 56\n",
            "Test 57\n",
            "Test 58\n",
            "Test 59\n",
            "Test 60\n",
            "Test 61\n",
            "Test 62\n",
            "Test 63\n",
            "Test 64\n",
            "Test 65\n",
            "Test 66\n",
            "Test 67\n",
            "Test 68\n",
            "Test 69\n",
            "Test 70\n",
            "Test 71\n",
            "Test 72\n",
            "Test 73\n",
            "Test 74\n",
            "Test 75\n",
            "Test 76\n",
            "Test 77\n",
            "Test 78\n",
            "Test 79\n",
            "Test 80\n",
            "Test 81\n",
            "Test 82\n",
            "Test 83\n",
            "Test 84\n",
            "Test 85\n",
            "Test 86\n",
            "Test 87\n",
            "Test 88\n",
            "Test 89\n",
            "Test 90\n",
            "Test 91\n",
            "Test 92\n",
            "Test 93\n",
            "Test 94\n",
            "Test 95\n",
            "Test 96\n",
            "Test 97\n",
            "Test 98\n",
            "Test 99\n",
            "100 out of 100 tests passed.\n",
            "0 out of 100 tests failed.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Initialize counters\n",
        "passed_tests = 0\n",
        "failed_tests = 0\n",
        "failed_shapes = []\n",
        "\n",
        "# Function to generate random dimensions\n",
        "def generate_random_dimensions():\n",
        "    M = random.randint(1, 2048)  # Rows in a\n",
        "    K = random.randint(1, 2048)  # Columns in a and rows in b\n",
        "    N = random.randint(1, 2048)  # Columns in b\n",
        "    M, K, N = 512, 512, 512\n",
        "    return M, K, N\n",
        "\n",
        "# Run 100 tests\n",
        "num = 100\n",
        "for i in range(num):\n",
        "    print(\"Test\", i)\n",
        "    # Generate random dimensions\n",
        "    M, K, N = generate_random_dimensions()\n",
        "\n",
        "    # Set a manual seed for reproducibility\n",
        "    torch.manual_seed(i)\n",
        "\n",
        "    # Generate random tensors with the generated dimensions\n",
        "    s = 0.1\n",
        "    a = torch.randn((M, K), device=torch.device(\"cuda\"), dtype=torch.float16) * s\n",
        "    b = torch.randn((K, N), device=torch.device(\"cuda\"), dtype=torch.float16) * s\n",
        "    c = torch.randn((M, N), device=torch.device(\"cuda\"), dtype=torch.float16) * s\n",
        "\n",
        "    # Compute outputs using Triton and reference implementations\n",
        "    triton_output = matmul_add_relu_fp16(a, b, c)\n",
        "    torch_output = reference_matmul_add_relu(a, b, c)\n",
        "\n",
        "    # Define relative tolerance\n",
        "    rtol = 1e-2 if is_hip_mi200() else 0.032\n",
        "\n",
        "    # Check if outputs are close within the specified tolerance\n",
        "    if torch.allclose(triton_output, torch_output, atol=0.15, rtol=rtol):\n",
        "        passed_tests += 1  # Increment counter if the test passes\n",
        "    else:\n",
        "        failed_tests += 1  # Increment counter if the test fails\n",
        "        failed_shapes.append((a.shape, b.shape, c.shape))  # Log the shapes\n",
        "\n",
        "# After the loop, print the number of passed and failed tests\n",
        "print(f\"{passed_tests} out of {num} tests passed.\")\n",
        "print(f\"{failed_tests} out of {num} tests failed.\")\n",
        "\n",
        "# Print the shapes of matrices that failed\n",
        "if failed_tests > 0:\n",
        "    print(\"Shapes of matrices that failed:\")\n",
        "    for idx, shapes in enumerate(failed_shapes):\n",
        "        a_shape, b_shape, c_shape = shapes\n",
        "        print(f\"Test {idx + 1}:\")\n",
        "        print(f\"  a.shape: {a_shape}\")\n",
        "        print(f\"  b.shape: {b_shape}\")\n",
        "        print(f\"  c.shape: {c_shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "K9Hdpxic0tq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f6adab3-a28c-45c4-acd9-eeba80b3ca8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: BLOCK_M=32, BLOCK_N=32, BLOCK_K=32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.35x\n",
            "Testing: BLOCK_M=32, BLOCK_N=32, BLOCK_K=64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.34x\n",
            "Testing: BLOCK_M=32, BLOCK_N=32, BLOCK_K=128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.34x\n",
            "Testing: BLOCK_M=32, BLOCK_N=32, BLOCK_K=512\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.32x\n",
            "Testing: BLOCK_M=32, BLOCK_N=64, BLOCK_K=32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.30x\n",
            "Testing: BLOCK_M=32, BLOCK_N=64, BLOCK_K=64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.31x\n",
            "Testing: BLOCK_M=32, BLOCK_N=64, BLOCK_K=128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.32x\n",
            "Testing: BLOCK_M=32, BLOCK_N=128, BLOCK_K=32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.32x\n",
            "Testing: BLOCK_M=32, BLOCK_N=128, BLOCK_K=64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.33x\n",
            "Testing: BLOCK_M=32, BLOCK_N=128, BLOCK_K=128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.33x\n",
            "Testing: BLOCK_M=32, BLOCK_N=512, BLOCK_K=32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.33x\n",
            "Testing: BLOCK_M=64, BLOCK_N=32, BLOCK_K=32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.32x\n",
            "Testing: BLOCK_M=64, BLOCK_N=32, BLOCK_K=64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.31x\n",
            "Testing: BLOCK_M=64, BLOCK_N=32, BLOCK_K=128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.31x\n",
            "Testing: BLOCK_M=64, BLOCK_N=64, BLOCK_K=32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.33x\n",
            "Testing: BLOCK_M=64, BLOCK_N=64, BLOCK_K=64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.32x\n",
            "Testing: BLOCK_M=64, BLOCK_N=64, BLOCK_K=128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.31x\n",
            "Testing: BLOCK_M=64, BLOCK_N=128, BLOCK_K=32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.32x\n",
            "Testing: BLOCK_M=64, BLOCK_N=128, BLOCK_K=64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.31x\n",
            "Testing: BLOCK_M=128, BLOCK_N=32, BLOCK_K=32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.32x\n",
            "Testing: BLOCK_M=128, BLOCK_N=32, BLOCK_K=64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.31x\n",
            "Testing: BLOCK_M=128, BLOCK_N=32, BLOCK_K=128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.32x\n",
            "Testing: BLOCK_M=128, BLOCK_N=64, BLOCK_K=32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.31x\n",
            "Testing: BLOCK_M=128, BLOCK_N=64, BLOCK_K=64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.32x\n",
            "Testing: BLOCK_M=128, BLOCK_N=128, BLOCK_K=32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.32x\n",
            "Testing: BLOCK_M=512, BLOCK_N=32, BLOCK_K=32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Speedup of Triton vs PyTorch: 1.31x\n",
            "\n",
            "Best parameters:\n",
            "BLOCK_M=32, BLOCK_N=32, BLOCK_K=32\n",
            "Best Triton time: 0.78 ms\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Performance Benchmark\n",
        "# IMPORTANT: DO NOT CHANGE THIS CODE.\n",
        "# THIS IS THE EXACT CODE THAT WILL BE USED TO GRADE YOUR IMPLEMENTATION.\n",
        "# ANY CHANGES TO THIS CODE (INCLUDING DIMENSIONS, REPEATS, etc.)\n",
        "# WILL CAUSE YOU TO HAVE DIFFERENT SPEEDUP RESULTS.\n",
        "# -----------------------------------------------------------------------------\n",
        "M = 2048\n",
        "K = 2048\n",
        "N = 2048\n",
        "\n",
        "# KEEP THESE MATRICES IN FP16. FP32 WILL NOT PROVIDE ACCURATE RESULTS\n",
        "A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "# warmup\n",
        "_ = matmul_add_relu_fp16(A, B, C)\n",
        "_ = reference_matmul_add_relu(A, B, C)\n",
        "\n",
        "REPEATS = 5000\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Grid Search for Best Parameters\n",
        "# -----------------------------------------------------------------------------\n",
        "best_time = float('inf')\n",
        "best_block_M = 0\n",
        "best_block_N = 0\n",
        "best_block_K = 0\n",
        "\n",
        "for block_M in [32, 64, 128, 512]:\n",
        "    for block_N in [32, 64, 128, 512]:\n",
        "        for block_K in [32, 64, 128, 512]:\n",
        "            if block_M*block_N*block_K > 128*128*32:\n",
        "                continue\n",
        "            # Print current parameters being tested\n",
        "            print(f\"Testing: BLOCK_M={block_M}, BLOCK_N={block_N}, BLOCK_K={block_K}\")\n",
        "\n",
        "            # Time your Triton implementation\n",
        "            print(\"Triton implementation\")\n",
        "            torch.cuda.synchronize()\n",
        "            start = time.perf_counter()\n",
        "            for _ in range(REPEATS):\n",
        "                _ = matmul_add_relu_fp16(A, B, C)\n",
        "            torch.cuda.synchronize()\n",
        "            triton_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "            # Time PyTorch implementation\n",
        "            print(\"PyTorch implementation\")\n",
        "            torch.cuda.synchronize()\n",
        "            start = time.perf_counter()\n",
        "            for _ in range(REPEATS):\n",
        "                _ = reference_matmul_add_relu(A, B, C)\n",
        "            torch.cuda.synchronize()\n",
        "            torch_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "            # Compare performance and store best time\n",
        "            speedup = torch_time / triton_time\n",
        "            print(f\"Speedup of Triton vs PyTorch: {speedup:.2f}x\")\n",
        "\n",
        "            if triton_time < best_time:\n",
        "                best_time = triton_time\n",
        "                best_block_M = block_M\n",
        "                best_block_N = block_N\n",
        "                best_block_K = block_K\n",
        "\n",
        "# Print the best performing block sizes\n",
        "print(f\"\\nBest parameters:\")\n",
        "print(f\"BLOCK_M={best_block_M}, BLOCK_N={best_block_N}, BLOCK_K={best_block_K}\")\n",
        "print(f\"Best Triton time: {best_time*1000:.2f} ms\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cse234",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}