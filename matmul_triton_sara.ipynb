{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sudhansh6/cse234-pa2/blob/main/matmul_triton_sara.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E-mNhUjQuxNM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "N9lmLw8cuxNN"
      },
      "outputs": [],
      "source": [
        "def is_cuda():\n",
        "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eUMlpjFJuxNO"
      },
      "outputs": [],
      "source": [
        "def is_hip_mi200():\n",
        "    target = triton.runtime.driver.active.get_current_target()\n",
        "    return target.backend == 'hip' and target.arch == 'gfx90a'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lBNGYaejuxNO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PA2 Part 2: MatMul+Relu+Add Fused Optimization.\n",
        "The kernel uses several optimization techniques:\n",
        "\n",
        "  1. Shared memory tiling.\n",
        "  2. Register tiling.\n",
        "  3. Cooperative fetching.\n",
        "  4. Operator Fusion\n",
        "  5. Write cache / epilogue fusion.\n",
        "\n",
        "Fill in the missing parts (marked with TODO).\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Tiling parameters - You will need to change these to achieve better results.\n",
        "# -----------------------------------------------------------------------------\n",
        "BLOCK_M = 128  # Tile size in the M dimension.\n",
        "BLOCK_N = 256 # Tile size in the N dimension.\n",
        "BLOCK_K = 32 # Tile size in the K dimension.\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Triton Kernel: Matrix Multiplication + ReLU + Add\n",
        "#\n",
        "# The kernel uses:\n",
        "#   Step 1: Tile assignment (each kernel computes a tile of C)\n",
        "#   Step 2: Shared memory tiling + Cooperative Fetching: Load tiles of A and B.\n",
        "#   Step 3: Register tiling: Use a register accumulator.\n",
        "#   Step 4: Add and ReLU fusion\n",
        "#   Step 5: Write cache/Epilogue: Write the final tile back to global memory.\n",
        "# -----------------------------------------------------------------------------\n",
        "@triton.jit\n",
        "def matmul_add_relu_kernel_fp16(\n",
        "    a_ptr, b_ptr, c_ptr, d_ptr,\n",
        "    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n",
        "    stride_am: tl.constexpr, stride_ak: tl.constexpr,\n",
        "    stride_bk: tl.constexpr, stride_bn: tl.constexpr,\n",
        "    stride_cm: tl.constexpr, stride_cn: tl.constexpr,\n",
        "    stride_dm: tl.constexpr, stride_dn: tl.constexpr,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 1: Tile: Assignment\n",
        "    #\n",
        "    # Each kernel instance is mapped to a tile in the output matrix C.\n",
        "    # Compute the starting indices (m_start, n_start) for this tile.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Compute the tile indices using program_id(0) for M and program_id(1) for N.\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    m_start = pid_m * BLOCK_M\n",
        "    n_start = pid_n * BLOCK_N\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 2: Register Tiling\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Initialize the accumulator \"acc\" with zeros (dtype: float16).\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float16)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 3: Shared Memory Tiling & Cooperative Fetching.\n",
        "    # Compute pointers to the sub-tiles of A and B that are needed to compute\n",
        "    # the current C tile. The offsets here serve to load BLOCK_SIZE_M x BLOCK_SIZE_K\n",
        "    # and BLOCK_SIZE_K x BLOCK_SIZE_N blocks from A and B respectively.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "    offs_am = m_start + tl.arange(0, BLOCK_M)[:, None]\n",
        "    offs_bn = n_start + tl.arange(0, BLOCK_N)[None, :]\n",
        "    for k in range(0, K, BLOCK_K):\n",
        "\n",
        "        offs_k = k + tl.arange(0, BLOCK_K)\n",
        "\n",
        "        a_ptrs = a_ptr + (offs_am * stride_am + offs_k * stride_ak)\n",
        "        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn * stride_bn)\n",
        "\n",
        "        a_tile = tl.load(a_ptrs, mask=(offs_am < M) & (offs_k < K), other=0.0)\n",
        "        b_tile = tl.load(b_ptrs, mask=(offs_k[:, None] < K) & (offs_bn < N), other=0.0)\n",
        "\n",
        "        acc += tl.dot(a_tile, b_tile, out_dtype=tl.float16)\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 4: Apply ReLU and Add C to the accumulator\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "    offs_cm = m_start + tl.arange(0, BLOCK_M)[:, None]\n",
        "    offs_cn = n_start + tl.arange(0, BLOCK_N)[None, :]\n",
        "    c_ptrs = c_ptr + stride_cm * offs_cm + stride_cn * offs_cn\n",
        "\n",
        "    acc = tl.maximum(acc + tl.load(c_ptrs, mask=(offs_cm < M) & (offs_cn < N), other=0.0), 0)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 5: Write Cache / Epilogue Fusion: Write the computed tile to D.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "    d_ptrs = d_ptr + stride_dm * offs_cm + stride_dn * offs_cn\n",
        "    tl.store(d_ptrs, acc, mask=(offs_cm < M) & (offs_cn < N))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "u16sz-IUuxNP"
      },
      "outputs": [],
      "source": [
        "def matmul_add_relu_fp16(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes Output = ReLU(A @ B + C) using fp16 precision for maximum throughput.\n",
        "    \"\"\"\n",
        "    M, K = a.shape\n",
        "    K2, N = b.shape\n",
        "    assert K == K2, \"Incompatible dimensions\"\n",
        "\n",
        "    d = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
        "    # Create launch grid\n",
        "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
        "\n",
        "    matmul_add_relu_kernel_fp16[grid](\n",
        "        a, b, c, d,\n",
        "        M, N, K,\n",
        "        a.stride(0), a.stride(1),\n",
        "        b.stride(0), b.stride(1),\n",
        "        c.stride(0), c.stride(1),\n",
        "        d.stride(0), d.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n",
        "    )\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AJ7LlTPawPqB"
      },
      "outputs": [],
      "source": [
        "# Reference implementation using PyTorch\n",
        "def reference_matmul_add_relu(A, B, C):\n",
        "    result = torch.matmul(A, B).add(C).relu_()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "B4J5ZBpOuxNP"
      },
      "outputs": [],
      "source": [
        "# # -----------------------------------------------------------------------------\n",
        "# # Accuracy Tests\n",
        "# # -----------------------------------------------------------------------------\n",
        "# if __name__ == \"__main__\":\n",
        "#     torch.manual_seed(0)\n",
        "#     a = torch.randn((1139, 3213), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "#     b = torch.randn((3213, 4215), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "#     c = torch.randn((1139, 4215), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "#     triton_output = matmul_add_relu_fp16(a, b, c)\n",
        "#     torch_output = reference_matmul_add_relu(a, b, c)\n",
        "#     print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
        "#     print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
        "#     rtol = 1e-2 if is_hip_mi200() else 0.032\n",
        "#     if torch.allclose(triton_output, torch_output, atol=0.15, rtol=rtol):\n",
        "#         print(\"✅ Triton and Torch match\")\n",
        "#     else:\n",
        "#         diff = triton_output - torch_output\n",
        "#         abs_diff = torch.abs(diff)\n",
        "#         max_abs_diff = torch.max(abs_diff)\n",
        "#         print(f\"❌ Triton and Torch differ: {max_abs_diff=}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # Normalize input values to prevent fp16 overflows\n",
        "    a = (torch.randn((1139, 3213), device=\"cuda\", dtype=torch.float16) * 0.1)\n",
        "    b = (torch.randn((3213, 4215), device=\"cuda\", dtype=torch.float16) * 0.1)\n",
        "    c = (torch.randn((1139, 4215), device=\"cuda\", dtype=torch.float16) * 0.1)\n",
        "\n",
        "    # Compute outputs\n",
        "    triton_output = matmul_add_relu_fp16(a, b, c)\n",
        "    torch_output = reference_matmul_add_relu(a, b, c)\n",
        "\n",
        "    # Print outputs for debugging\n",
        "    print(f\"Triton Output:\\n {triton_output}\")\n",
        "    print(f\"Torch Output:\\n {torch_output}\")\n",
        "\n",
        "    rtol = 1e-2 if is_hip_mi200() else 0.032\n",
        "    if torch.allclose(triton_output, torch_output, atol=0.15, rtol=rtol):\n",
        "        print(\"✅ Triton and Torch match\")\n",
        "    else:\n",
        "        diff = triton_output - torch_output\n",
        "        abs_diff = torch.abs(diff)\n",
        "        max_abs_diff = torch.max(abs_diff)\n",
        "        print(f\"❌ Triton and Torch differ: {max_abs_diff=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRwWDp6ye2PS",
        "outputId": "f5064937-eecd-4117-abec-1f8ad4563de5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triton Output:\n",
            " tensor([[0.2659, 0.0644, 1.0361,  ..., 0.6006, 0.0045, 0.0000],\n",
            "        [0.6382, 0.0000, 1.1904,  ..., 0.2001, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.1382, 0.0000, 0.3589],\n",
            "        ...,\n",
            "        [0.0000, 0.1774, 0.0000,  ..., 0.2593, 0.0639, 0.3738],\n",
            "        [0.2800, 0.0000, 0.5938,  ..., 0.0000, 0.5625, 0.6572],\n",
            "        [0.5381, 0.0000, 0.4954,  ..., 0.0000, 0.8140, 0.0000]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "Torch Output:\n",
            " tensor([[0.2668, 0.0653, 1.0322,  ..., 0.6006, 0.0046, 0.0000],\n",
            "        [0.6387, 0.0000, 1.1836,  ..., 0.2008, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.1393, 0.0000, 0.3564],\n",
            "        ...,\n",
            "        [0.0000, 0.1783, 0.0000,  ..., 0.2583, 0.0638, 0.3735],\n",
            "        [0.2795, 0.0000, 0.5938,  ..., 0.0000, 0.5625, 0.6558],\n",
            "        [0.5386, 0.0000, 0.4944,  ..., 0.0000, 0.8115, 0.0000]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "✅ Triton and Torch match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj_dGOlazQJY",
        "outputId": "c0e6582a-99e6-4a82-f8fe-79387567e9e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.75 ms\n",
            "PyTorch implementation: 1.04 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.39x\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Performance Benchmark\n",
        "# IMPORTANT: DO NOT CHANGE THIS CODE.\n",
        "# THIS IS THE EXACT CODE THAT WILL BE USED TO GRADE YOUR IMPLEMENTATION.\n",
        "# ANY CHANGES TO THIS CODE (INCLUDING DIMENSIONS, REPEATS, etc.)\n",
        "# WILL CAUSE YOU TO HAVE DIFFERENT SPEEDUP RESULTS.\n",
        "# -----------------------------------------------------------------------------\n",
        "M = 2048\n",
        "K = 2048\n",
        "N = 2048\n",
        "\n",
        "# KEEP THESE MATRICES IN FP16. FP32 WILL NOT PROVIDE ACCURATE RESULTS\n",
        "A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "# warmup\n",
        "_ = matmul_add_relu_fp16(A, B, C)\n",
        "_ = reference_matmul_add_relu(A, B, C)\n",
        "\n",
        "REPEATS = 5000\n",
        "\n",
        "# time your implementation\n",
        "print(\"Triton implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = matmul_add_relu_fp16(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "triton_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "# time pytorch\n",
        "print(\"PyTorch implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = reference_matmul_add_relu(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "torch_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "print(f\"Performance comparison for matrix multiplication ({M}x{K} @ {K}x{N}):\")\n",
        "print(f\"Triton implementation: {triton_time*1000:.2f} ms\")\n",
        "print(f\"PyTorch implementation: {torch_time*1000:.2f} ms\")\n",
        "\n",
        "print(f\"\\nSpeedup of Triton vs PyTorch: {torch_time/triton_time:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Initialize counters\n",
        "passed_tests = 0\n",
        "failed_tests = 0\n",
        "failed_shapes = []\n",
        "\n",
        "# Function to generate random dimensions\n",
        "def generate_random_dimensions():\n",
        "    M = random.randint(1, 2048)  # Rows in a\n",
        "    K = random.randint(1, 2048)  # Columns in a and rows in b\n",
        "    N = random.randint(1, 2048)  # Columns in b\n",
        "    return M, K, N\n",
        "\n",
        "# Run 100 tests\n",
        "for i in range(50):\n",
        "    print(\"Test\", i)\n",
        "    # Generate random dimensions\n",
        "    M, K, N = generate_random_dimensions()\n",
        "\n",
        "    # Set a manual seed for reproducibility\n",
        "    torch.manual_seed(i)\n",
        "\n",
        "    # Generate random tensors with the generated dimensions\n",
        "    s = 0.1\n",
        "    a = torch.randn((M, K), device=torch.device(\"cuda\"), dtype=torch.float16) * s\n",
        "    b = torch.randn((K, N), device=torch.device(\"cuda\"), dtype=torch.float16) * s\n",
        "    c = torch.randn((M, N), device=torch.device(\"cuda\"), dtype=torch.float16) * s\n",
        "\n",
        "    # Compute outputs using Triton and reference implementations\n",
        "    triton_output = matmul_add_relu_fp16(a, b, c)\n",
        "    torch_output = reference_matmul_add_relu(a, b, c)\n",
        "\n",
        "    # Define relative tolerance\n",
        "    rtol = 1e-2 if is_hip_mi200() else 0.032\n",
        "\n",
        "    # Check if outputs are close within the specified tolerance\n",
        "    if torch.allclose(triton_output, torch_output, atol=0.15, rtol=rtol):\n",
        "        passed_tests += 1  # Increment counter if the test passes\n",
        "    else:\n",
        "        failed_tests += 1  # Increment counter if the test fails\n",
        "        failed_shapes.append((a.shape, b.shape, c.shape))  # Log the shapes\n",
        "\n",
        "# After the loop, print the number of passed and failed tests\n",
        "print(f\"{passed_tests} out of 50 tests passed.\")\n",
        "print(f\"{failed_tests} out of 50 tests failed.\")\n",
        "\n",
        "# Print the shapes of matrices that failed\n",
        "if failed_tests > 0:\n",
        "    print(\"Shapes of matrices that failed:\")\n",
        "    for idx, shapes in enumerate(failed_shapes):\n",
        "        a_shape, b_shape, c_shape = shapes\n",
        "        print(f\"Test {idx + 1}:\")\n",
        "        print(f\"  a.shape: {a_shape}\")\n",
        "        print(f\"  b.shape: {b_shape}\")\n",
        "        print(f\"  c.shape: {c_shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeU0ziM8iYRx",
        "outputId": "5133f8ff-d05b-4108-a613-ff912038f76c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 0\n",
            "Test 1\n",
            "Test 2\n",
            "Test 3\n",
            "Test 4\n",
            "Test 5\n",
            "Test 6\n",
            "Test 7\n",
            "Test 8\n",
            "Test 9\n",
            "Test 10\n",
            "Test 11\n",
            "Test 12\n",
            "Test 13\n",
            "Test 14\n",
            "Test 15\n",
            "Test 16\n",
            "Test 17\n",
            "Test 18\n",
            "Test 19\n",
            "Test 20\n",
            "Test 21\n",
            "Test 22\n",
            "Test 23\n",
            "Test 24\n",
            "Test 25\n",
            "Test 26\n",
            "Test 27\n",
            "Test 28\n",
            "Test 29\n",
            "Test 30\n",
            "Test 31\n",
            "Test 32\n",
            "Test 33\n",
            "Test 34\n",
            "Test 35\n",
            "Test 36\n",
            "Test 37\n",
            "Test 38\n",
            "Test 39\n",
            "Test 40\n",
            "Test 41\n",
            "Test 42\n",
            "Test 43\n",
            "Test 44\n",
            "Test 45\n",
            "Test 46\n",
            "Test 47\n",
            "Test 48\n",
            "Test 49\n",
            "50 out of 50 tests passed.\n",
            "0 out of 50 tests failed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GRID SEARCH\n",
        "\n",
        "\n",
        "BLOCK_M_VALUES = [64, 128]\n",
        "BLOCK_N_VALUES = [128, 256]\n",
        "BLOCK_K_VALUES = [32, 64]\n",
        "\n",
        "def is_safe_config(BLOCK_M, BLOCK_N, BLOCK_K):\n",
        "    shared_mem_size = (BLOCK_M * BLOCK_K + BLOCK_K * BLOCK_N) * 2\n",
        "    return shared_mem_size <= (48 * 1024)\n",
        "\n",
        "def benchmark_triton(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, repeats=5000):\n",
        "    A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "    B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "    C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "    D = torch.empty((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
        "\n",
        "    for _ in range(50):\n",
        "        matmul_add_relu_kernel_fp16[grid](\n",
        "            A, B, C, D,\n",
        "            M, N, K,\n",
        "            A.stride(0), A.stride(1),\n",
        "            B.stride(0), B.stride(1),\n",
        "            C.stride(0), C.stride(1),\n",
        "            D.stride(0), D.stride(1),\n",
        "            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n",
        "        )\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(repeats):\n",
        "        matmul_add_relu_kernel_fp16[grid](\n",
        "            A, B, C, D,\n",
        "            M, N, K,\n",
        "            A.stride(0), A.stride(1),\n",
        "            B.stride(0), B.stride(1),\n",
        "            C.stride(0), C.stride(1),\n",
        "            D.stride(0), D.stride(1),\n",
        "            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n",
        "        )\n",
        "    torch.cuda.synchronize()\n",
        "    elapsed_time = (time.perf_counter() - start) / repeats\n",
        "    return elapsed_time\n",
        "\n",
        "M, N, K = 2048, 2048, 2048\n",
        "best_time = float(\"inf\")\n",
        "best_config = None\n",
        "\n",
        "for BLOCK_M in BLOCK_M_VALUES:\n",
        "    for BLOCK_N in BLOCK_N_VALUES:\n",
        "        for BLOCK_K in BLOCK_K_VALUES:\n",
        "            if is_safe_config(BLOCK_M, BLOCK_N, BLOCK_K):\n",
        "                exec_time = benchmark_triton(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K)\n",
        "                print(f\"BLOCK_M={BLOCK_M}, BLOCK_N={BLOCK_N}, BLOCK_K={BLOCK_K} -> Time: {exec_time:.4f} ms\")\n",
        "                if exec_time < best_time:\n",
        "                    best_time = exec_time\n",
        "                    best_config = (BLOCK_M, BLOCK_N, BLOCK_K)\n",
        "\n",
        "print(f\"Best Config: BLOCK_M={best_config[0]}, BLOCK_N={best_config[1]}, BLOCK_K={best_config[2]} with time {best_time:.4f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyWSWBkCb6bt",
        "outputId": "eb57dca0-48fe-43bb-85d3-029cbed027db"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLOCK_M=64, BLOCK_N=128, BLOCK_K=32 -> Time: 0.0012 ms\n",
            "BLOCK_M=64, BLOCK_N=128, BLOCK_K=64 -> Time: 0.0012 ms\n",
            "BLOCK_M=64, BLOCK_N=256, BLOCK_K=32 -> Time: 0.0011 ms\n",
            "BLOCK_M=64, BLOCK_N=256, BLOCK_K=64 -> Time: 0.0013 ms\n",
            "BLOCK_M=128, BLOCK_N=128, BLOCK_K=32 -> Time: 0.0011 ms\n",
            "BLOCK_M=128, BLOCK_N=128, BLOCK_K=64 -> Time: 0.0009 ms\n",
            "BLOCK_M=128, BLOCK_N=256, BLOCK_K=32 -> Time: 0.0008 ms\n",
            "BLOCK_M=128, BLOCK_N=256, BLOCK_K=64 -> Time: 0.0009 ms\n",
            "Best Config: BLOCK_M=128, BLOCK_N=256, BLOCK_K=32 with time 0.0008 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hF6b7GFAhPuz"
      },
      "execution_count": 24,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "cse234",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}